{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fc9086-93aa-4645-8ba2-380c3acbbed9",
   "metadata": {},
   "source": [
    "# Level 1: Simple RAG\n",
    "\n",
    "This notebook will show you how to build a simple RAG application with Llama Stack. You will learn how the API's provided by Llama Stack can be used to directly control and invoke all common RAG stages, including indexing, retrieval and inference. \n",
    "\n",
    "_Note: This notebook contains a non-agentic implementation of RAG. We will show you how to build an agentic RAG application later in this tutorial in [Level4_agentic_RAG](Level4_agentic_RAG.ipynb)._\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "1. Indexing a collection of documents into a vector database for later retrieval.\n",
    "2. Executing the built-in RAG tool to retrieve the document chunks relevant to a given query.\n",
    "3. Using the retrieved context to answer user queries during the inference step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db34e4b-ed29-4007-b760-59543d4caca1",
   "metadata": {},
   "source": [
    "## 1. Setting Up this Notebook\n",
    "\n",
    "First, we will start with a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15080a6-48be-4475-8813-c584701d69bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from llama_stack_client import RAGDocument\n",
    "from llama_stack_client.types.shared.content_delta import TextDelta, ToolCallDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e8c70-6f28-440b-b71a-85d4040ffac4",
   "metadata": {},
   "source": [
    "Next, we will initialize our environment as described in detail in our [\"Getting Started\" notebook](Level0_getting_started_with_Llama_Stack.ipynb). Please refer to it for additional explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558909bb-955c-40a3-a0c2-1f4acb0dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Llama Stack server\n",
      "Inference Parameters:\n",
      "\tModel: ibm-granite/granite-3.2-8b-instruct\n",
      "\tSampling Parameters: {'strategy': {'type': 'greedy'}, 'max_tokens': 4096}\n",
      "\tstream: True\n"
     ]
    }
   ],
   "source": [
    "# for accessing the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# pretty print of the results returned from the model/agent\n",
    "import sys\n",
    "sys.path.append('..')  \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "\n",
    "base_url = os.getenv(\"REMOTE_BASE_URL\")\n",
    "\n",
    "# Tavily search API key is required for some of our demos and must be provided to the client upon initialization.\n",
    "# We will cover it in the agentic demos that use the respective tool. Please ignore this parameter for all other demos.\n",
    "tavily_search_api_key = os.getenv(\"TAVILY_SEARCH_API_KEY\")\n",
    "if tavily_search_api_key is None:\n",
    "    provider_data = None\n",
    "else:\n",
    "    provider_data = {\"tavily_search_api_key\": tavily_search_api_key}\n",
    "\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "    \n",
    "print(f\"Connected to Llama Stack server\")\n",
    "\n",
    "# model_id will later be used to pass the name of the desired inference model to Llama Stack Agents/Inference APIs\n",
    "model_id = os.getenv(\"INFERENCE_MODEL_ID\")\n",
    "\n",
    "temperature = float(os.getenv(\"TEMPERATURE\", 0.0))\n",
    "if temperature > 0.0:\n",
    "    top_p = float(os.getenv(\"TOP_P\", 0.95))\n",
    "    strategy = {\"type\": \"top_p\", \"temperature\": temperature, \"top_p\": top_p}\n",
    "else:\n",
    "    strategy = {\"type\": \"greedy\"}\n",
    "\n",
    "max_tokens = int(os.getenv(\"MAX_TOKENS\", 4096))\n",
    "\n",
    "# sampling_params will later be used to pass the parameters to Llama Stack Agents/Inference APIs\n",
    "sampling_params = {\n",
    "    \"strategy\": strategy,\n",
    "    \"max_tokens\": max_tokens,\n",
    "}\n",
    "\n",
    "stream_env = os.getenv(\"STREAM\", \"True\")\n",
    "# the Boolean 'stream' parameter will later be passed to Llama Stack Agents/Inference APIs\n",
    "# any value non equal to 'False' will be considered as 'True'\n",
    "stream = (stream_env != \"False\")\n",
    "\n",
    "print(f\"Inference Parameters:\\n\\tModel: {model_id}\\n\\tSampling Parameters: {sampling_params}\\n\\tstream: {stream}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eaadf-f5ac-4d7c-bb9d-f039ccd8d9a3",
   "metadata": {},
   "source": [
    "Finally, we complete the setup by initializing the document collection we will use for RAG ingestion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c725c2da-05e5-474f-9a44-cf5615557665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_db_id = f\"test_vector_db_{uuid.uuid4()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510929-fe4b-428c-8f9e-14d47a03daa2",
   "metadata": {},
   "source": [
    "## 2. Indexing the Documents\n",
    "- Initialize a new document collection in our vector database. All parameters related to the vector database, such as the embedding model and dimension, must be specified here.\n",
    "- Provide a list of document URLs to the RAG tool. Llama Stack will handle fetching, converting, and chunking the content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d81ffb2-2089-4cb8-adae-f32965f206c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define and register the document collection to be used\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=os.getenv(\"VDB_EMBEDDING\"),\n",
    "    embedding_dimension=int(os.getenv(\"VDB_EMBEDDING_DIMENSION\", 384)),\n",
    "    provider_id=os.getenv(\"VDB_PROVIDER\"),\n",
    ")\n",
    "\n",
    "# ingest the documents into the newly created document collection\n",
    "urls = [\n",
    "    (\"https://www.openshift.guide/openshift-guide-screen.pdf\", \"application/pdf\"),\n",
    "]\n",
    "documents = [\n",
    "    RAGDocument(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=url,\n",
    "        mime_type=url_type,\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, (url, url_type) in enumerate(urls)\n",
    "]\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=int(os.getenv(\"VECTOR_DB_CHUNK_SIZE\", 512)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5639413-90d6-42ae-add4-6c89da0297e2",
   "metadata": {},
   "source": [
    "## 3. Executing Queries via the Built-in RAG Tool\n",
    "- Directly invoke the RAG tool to query the vector database we ingested into at the previous stage.\n",
    "- Construct an extended prompt using the retrieved chunks.\n",
    "- Query the model with the extended prompt.\n",
    "- Output the reply received from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d39ab00-2a65-4b72-b5ed-4dd61f1204a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "User> How do I install OpenShift?\u001b[0m\n",
      "\u001b[33minference> \u001b[0m\u001b[33m\u001b[0m\u001b[33mTo\u001b[0m\u001b[33m install\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m follow\u001b[0m\u001b[33m these\u001b[0m\u001b[33m steps\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m1\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Launch\u001b[0m\u001b[33m your\u001b[0m\u001b[33m CRC\u001b[0m\u001b[33m instance\u001b[0m\u001b[33m and\u001b[0m\u001b[33m open\u001b[0m\u001b[33m the\u001b[0m\u001b[33m web\u001b[0m\u001b[33m console\u001b[0m\u001b[33m in\u001b[0m\u001b[33m your\u001b[0m\u001b[33m preferred\u001b[0m\u001b[33m web\u001b[0m\u001b[33m browser\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m2\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Select\u001b[0m\u001b[33m the\u001b[0m\u001b[33m \"\u001b[0m\u001b[33mDeveloper\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m perspective\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m3\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Click\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m \"\u001b[0m\u001b[33mAdd\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m entry\u001b[0m\u001b[33m in\u001b[0m\u001b[33m the\u001b[0m\u001b[33m left\u001b[0m\u001b[33m-hand\u001b[0m\u001b[33m side\u001b[0m\u001b[33m menu\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m4\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Select\u001b[0m\u001b[33m the\u001b[0m\u001b[33m \"\u001b[0m\u001b[33mContainer\u001b[0m\u001b[33m Image\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m option\u001b[0m\u001b[33m to\u001b[0m\u001b[33m deploy\u001b[0m\u001b[33m a\u001b[0m\u001b[33m container\u001b[0m\u001b[33m from\u001b[0m\u001b[33m a\u001b[0m\u001b[33m standard\u001b[0m\u001b[33m container\u001b[0m\u001b[33m registry\u001b[0m\u001b[33m.\n",
      "\u001b[0m\u001b[33m5\u001b[0m\u001b[33m.\u001b[0m\u001b[33m Enter\u001b[0m\u001b[33m the\u001b[0m\u001b[33m URL\u001b[0m\u001b[33m of\u001b[0m\u001b[33m the\u001b[0m\u001b[33m ready\u001b[0m\u001b[33m-to\u001b[0m\u001b[33m-use\u001b[0m\u001b[33m container\u001b[0m\u001b[33m and\u001b[0m\u001b[33m click\u001b[0m\u001b[33m on\u001b[0m\u001b[33m the\u001b[0m\u001b[33m \"\u001b[0m\u001b[33mCreate\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m button\u001b[0m\u001b[33m.\n",
      "\n",
      "\u001b[0m\u001b[33mAlternatively\u001b[0m\u001b[33m,\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m also\u001b[0m\u001b[33m use\u001b[0m\u001b[33m other\u001b[0m\u001b[33m methods\u001b[0m\u001b[33m such\u001b[0m\u001b[33m as\u001b[0m\u001b[33m:\n",
      "\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Using\u001b[0m\u001b[33m the\u001b[0m\u001b[33m Developer\u001b[0m\u001b[33m Catalog\u001b[0m\u001b[33m to\u001b[0m\u001b[33m browse\u001b[0m\u001b[33m and\u001b[0m\u001b[33m choose\u001b[0m\u001b[33m from\u001b[0m\u001b[33m available\u001b[0m\u001b[33m databases\u001b[0m\u001b[33m,\u001b[0m\u001b[33m message\u001b[0m\u001b[33m queues\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and\u001b[0m\u001b[33m other\u001b[0m\u001b[33m components\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Spec\u001b[0m\u001b[33mifying\u001b[0m\u001b[33m the\u001b[0m\u001b[33m URL\u001b[0m\u001b[33m of\u001b[0m\u001b[33m a\u001b[0m\u001b[33m source\u001b[0m\u001b[33m code\u001b[0m\u001b[33m project\u001b[0m\u001b[33m stored\u001b[0m\u001b[33m on\u001b[0m\u001b[33m a\u001b[0m\u001b[33m Git\u001b[0m\u001b[33m repository\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m*\u001b[0m\u001b[33m Import\u001b[0m\u001b[33ming\u001b[0m\u001b[33m YAML\u001b[0m\u001b[33m directly\u001b[0m\u001b[33m or\u001b[0m\u001b[33m even\u001b[0m\u001b[33m a\u001b[0m\u001b[33m J\u001b[0m\u001b[33mAR\u001b[0m\u001b[33m file\u001b[0m\u001b[33m with\u001b[0m\u001b[33m a\u001b[0m\u001b[33m Java\u001b[0m\u001b[33m application\u001b[0m\u001b[33m\n",
      "\n",
      "\u001b[0m\u001b[33mNote\u001b[0m\u001b[33m:\u001b[0m\u001b[33m You\u001b[0m\u001b[33m need\u001b[0m\u001b[33m to\u001b[0m\u001b[33m have\u001b[0m\u001b[33m Open\u001b[0m\u001b[33mShift\u001b[0m\u001b[33m installed\u001b[0m\u001b[33m and\u001b[0m\u001b[33m running\u001b[0m\u001b[33m before\u001b[0m\u001b[33m you\u001b[0m\u001b[33m can\u001b[0m\u001b[33m install\u001b[0m\u001b[33m it\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\u001b[0m"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"How do I install OpenShift?\",\n",
    "]\n",
    "\n",
    "for prompt in queries:\n",
    "    cprint(f\"\\nUser> {prompt}\", \"blue\")\n",
    "    \n",
    "    # RAG retrieval call\n",
    "    rag_response = client.tool_runtime.rag_tool.query(content=prompt, vector_db_ids=[vector_db_id])\n",
    "\n",
    "    # the list of messages to be sent to the model must start with the system prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "    # construct the actual prompt to be executed, incorporating the original query and the retrieved content\n",
    "    prompt_context = rag_response.content\n",
    "    extended_prompt = f\"Please answer the given query using the context below.\\n\\nCONTEXT:\\n{prompt_context}\\n\\nQUERY:\\n{prompt}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": extended_prompt})\n",
    "\n",
    "    # use Llama Stack inference API to directly communicate with the desired model\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=messages,\n",
    "        model_id=model_id,\n",
    "        sampling_params=sampling_params,\n",
    "        stream=stream,\n",
    "    )\n",
    "    \n",
    "    # print the response\n",
    "    cprint(\"inference> \", color=\"yellow\", end='')\n",
    "    for chunk in response:\n",
    "        response_delta = chunk.event.delta\n",
    "        if isinstance(response_delta, TextDelta):\n",
    "            cprint(response_delta.text, color=\"yellow\", end='')\n",
    "        elif isinstance(response_delta, ToolCallDelta):\n",
    "            cprint(response_delta.tool_call, color=\"yellow\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6937a3-3efa-4b66-aaf0-85d96b6d43db",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "This notebook demonstrated how to set up and use the built-in RAG tool for ingesting user-provided documents in a vector database and utilizing them during inference via direct retrieval. \n",
    "\n",
    "Now that we've seen how easy it is to implement RAG with Llama Stack, We'll move on to building a simple agent with Llama Stack next in our [Simple Agents](./Level2_simple_agentic_with_websearch.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
